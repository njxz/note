# Adaboost 

在overview中，集成学习按照个弱学习器之间是否存在依赖关系分成了两类，

一类是个体学习器之间存在依赖关系

另一类是个体学习器之间不存在依赖关系

前者的算法代表室boosting系列算法，Adaboost是最著名的算法之一，Adaboost既可以用作分类，也可以用作回归。



## 1.回复boosting的基本原理

![img](https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204194331365-2142863547.png)

​	从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。　　

　　不过有几个具体的问题Boosting算法没有详细说明。

1.  如何计算学习误差率e?
2. 如何得到弱学习器权重系数α α?
3. 如何更新样本权重D?
4. 使用何种结合策略？

　　只要是boosting大家族的算法，都要解决这4个问题。那么Adaboost是怎么解决的呢？



## 2.Adaboost算法的基本思路

如下是Adaboost解决上一节四个问题的：

假设训练样本：

​	$$T={(x , y_1 ),(x_2 ,y_2 ),...(x_m ,y_m )} $$

训练集的第K个弱学习器的输出权重：

​	$$D(k)=(w_{k1} ,w_{k2} ,...w_{km} );w_{1i} =\frac1m ;i=1,2...m $$

首先看下Adaboost的分类问题



