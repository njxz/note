---

---

# Ensemble Learn（集成学习）

[TOC]



原文出处：[刘建平Pinard](http://www.cnblogs.com/pinard/p/6131423.html)

## 集成学习的概念：

> 它本身不是一个单独的机器学习算法，而是通过构建并合并多个机器学习器来完成学习任务的。

## 集成学习使用地方：

集成学习可以用于：

1. 分类问题集成
2. 回归问题集成
3. 特征选取集成
4. 异常点检测集成

可以说所有的机器学习领域都可以看到集成学习的身影。

## 1.集成学习概述



![img](https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204191919974-1029671964.png)



上图概括了集成学习的思想。对于训练集数据，我们通过训练若干个学习器，通过一定的策略，最终姓曾一个强学习期。

**需要解决的问题：**

1. 如何得到若干个个体学习器
2. 如何选择一种结合策略来变成一个强学习器


## 2.集成学习之个体学习器

对于得到个体学习器有两种选择：

1. **同质学习器**：所有的学习器都是一个种类，比如都是策略书个体学习器，或者神经网络个体学习器。

   同质学习器使用比较广泛的模型有**CART决策树**和**神经网络**

   ​

2. **异质学习器**：学习器不全是一个种类，例如一个分类问题，对训练集才用支持向量集，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过结合策略来结合。

目前来说，同质学习方法的使用较为广泛。

同质学习器又可以根据个体学习器之间是否存在依赖关系分为两类：

1. 个体学习器之间存在强依赖关系：个体学习器基本都需要串行生成，代表算法是boosting系列算法
2. 个体学习器之间不存在依赖关系：一系列个体学习器可以并行生成，代表算法是bagging和随机森林（Random Forest）系列算法。


##  3.集成学习之boosting

boosting的算法原理可以用一个图概括：

![img](https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204194331365-2142863547.png)

boosting算法的工作机制是首先从训练集用**初始权重**训练出一个**弱学习器1**，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

​	Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。AdaBoost和提升树算法的原理在后面的文章中会专门来讲



## 4.集成学习之bagging

bagging的原理和boosting不同，他的弱学习器之间没有依赖关系，可以并行生成。![img](https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204200000787-1988863729.png)

从图可知，每个弱学习器的样本是通过随机采样获得的。通过T次随机抽样，我们可以得到T个采样集，对于这T个采样集，我们可以分别训练出T个弱学习器，在对这个弱学习器通过集合策略来得到最终的强学习器

对于这里的随机采样方法：

自助采样法（Bootstap sampling）对于M各样本的原始训练集，每次随机采取一个样本再放回，这样采集m次，最终可以得到m个样本的采样集。由于是随机采样，这样每次的采样集都和原始训练集不同，和其他的采集器不同，这样便得到了不同的弱学习器。

随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。



## 5.集成学习之结合策略

首先，假设T个弱学习器是{h1,h2,h3,……hT}，最终的预测为H(x)

### 5.1 平均法

> ​	对若干个弱学习器的输出进行平均得到最终的预测输出

对于数值类的回归预测问题，通常使用$\color{red} 平均法$。

最简单的算数平均：

​		$$ H(x)={\frac 1 T}\sum_{i=1}^T h_i(x)$$

如果每个个体有一个权重w，则最终预测是：

​		$$H(x)=\sum_{i=1}^T w_ih_i(x)$$

其中$w_i$是个体学习器$h_i$的权重

​		$$W_i\ge0,\sum_{i=1}^Tw_i=1$$



### 5.2 投票法

假设预测类别是{c1,c2,……ck}，对于任意个预测样本x,我们的T个弱学习器的预测结果分别是($h_1(x),h_2(x)...h_k(x)$)

对于$\color{red} 分类问题的预测$,通常使用投票法

最简单的是$\color{red}相对多数投票法$：从T个弱学习器对样本x的预测结果中，数量最多的类别$c_i$将会为最终的识别类别。如果不止一个获得最高配，则随机选择一个。

稍微复杂的$\color{red}绝对多数投票法$:在相对多数投票的基础上，不光要最高，还要票过半数

更加复杂的是$\color{red}加权投票法$：每个弱学习器的屁分类票数乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别



### 5.3 学习法

上面的学习方法比较简单，但是可能学习误差较大，于是就有了学习法。代表方法是stacking,我们在弱学习器的结果上再加上一层学习器，将训练器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来获得最终结果。

这里我们将弱学习器称为初级学习器，后面的学习器成为次级学习器。

对于测试集，我们首先用初级学习器预测一次，在用结果作为次级学习器的输入再预测一次获得最终的预测结果。





